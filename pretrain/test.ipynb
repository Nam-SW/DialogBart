{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T06:55:12.510142Z",
     "start_time": "2022-03-16T06:55:09.846560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:01:11.480944: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nsw0311/nas_storage/etc/DialogBPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T06:55:22.451471Z",
     "start_time": "2022-03-16T06:55:12.511783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97303687b77a4ef3b30a3b692f6c46b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataloader import load\n",
    "\n",
    "config = {\n",
    "    \"train_data_path\": \"/home/nsw0311/nas_storage/datasets/RG/dialog_data.json\",\n",
    "    \"eval_data_path\": None,\n",
    "    \"train_test_split\": None,\n",
    "    \"worker\": 10,\n",
    "    \"batch_size\": 1000,\n",
    "    \"shuffle_seed\": None,\n",
    "    \"seq_len\": 512,\n",
    "    \"mask_ratio\": 0.3,\n",
    "    \"random_ratio\": 0.5,\n",
    "    \"insert_ratio\": 0.2,\n",
    "    \"rotate_ratio\": 0.2,\n",
    "    \"permute_sentence_ratio\": 1.0\n",
    "}\n",
    "\n",
    "train_dataset, eval_dataset = load(\n",
    "    tokenizer=tokenizer, \n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T07:07:46.066479Z",
     "start_time": "2022-03-16T07:07:46.056168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   10, 17425,  7103, 11067,   146,    11,  7121, 10130,   177,  4831,\n",
       "           283,  1438,  7204,  7647,  7196,  7143,   438,  7954,  7242, 15537,\n",
       "          8737, 11940, 16379, 16134,     4,  7211,    11,  7071,     4,  2562,\n",
       "           989,     4,  5025,     4,  7680,    10,  7707,  9541,    10,  7107,\n",
       "          7854,  7439, 10460,     4,  7069,  7048, 19176,  4590,  7413,  9040,\n",
       "         11867,    10,   225,  9117,  7068,  5474,  1418,  7107,     4,  2150,\n",
       "             4, 18198,  8342,    11,  3773,  2463,     4,  4395,    11,    11,\n",
       "             4,     4,  7131,  8019,  7419,   567,  3888,  7298,  7295,   245,\n",
       "            11, 10638,  7120,     4,  4212,   176,   206,  7019,  7099,  7068,\n",
       "           177,  9079,  2505,    10,  3773,     4,  2463,  7130,  1121,  1121,\n",
       "             4,   228,  1034,  7940,  4707,  4800,    10,  7401,  4188,  4280,\n",
       "          4188,  4305, 11852,  7215,    11,  7075,  4489,    11,  8074,  7464,\n",
       "          7685,  7084,  9102,  8682,  8258,  7062,  7544,    11,  6999,    11,\n",
       "         16166,  4632,  6999,  2224,  5025,  2126,     4, 10337,    10, 13999,\n",
       "         16837,  7493,  7003,    10,  7126,  7964,  4468,  5868,  4468,    10,\n",
       "          7049,  7471,   177,  1903,  1418,  7274,     4,    10,  7484,  7003,\n",
       "          7031,  7132,  8686, 11297,  7098,  2093,    11,  7143,  7094, 11746,\n",
       "          7760,    11,  4253,   232, 12012,   232,   218, 16336,  9862,     4,\n",
       "          4632,   248,    10,  8063, 14860,  7501,  7683,  7186,  7088,  7493,\n",
       "          7016, 18208,    11,  7090,  7257,  7023,  7056, 12250,  7155, 14496,\n",
       "          6633,    11,  7043,  7087, 11384, 10773,    10,    11,  7047,  8936,\n",
       "          7389,    10,  7240,  7121,     4,    11,   248, 19957,    11,  7023,\n",
       "          7143, 11956,  7692,    11,  7047,  9104,  4607,    10,     4, 11884,\n",
       "           128,   128,  7522,    10, 14437, 10638,  7861,    11,  7175,  5045,\n",
       "         19137,     4, 14415,  7814,  5094,     4,   146,    10,  7102,  2846,\n",
       "          9878,     4,     4,  4696,  7108,  6992,  7514,   611,  2463,  8858,\n",
       "         11128,  7830,     4,  7412,  6991,     4,  7086,    11,  6633, 17483,\n",
       "         19176,  1571,  7406,     4,  3832, 11387,    10, 11449, 17871,  9981,\n",
       "           146,    10,  6995, 10397,  6632,  6632, 11025,  7020,  8647,    11,\n",
       "          7023,  4632,    11,  7091,  8478,  5868, 16154,    11,  7046,  6447,\n",
       "          7010,  7143,  7010, 11656,  2354,  1053,    11, 17967,    10,  7075,\n",
       "          4586,  7001, 17466,   177, 13026,  3323, 13083,  6541,    10,  7991,\n",
       "             4,  4160,  8936,    10, 11646,     4,   177,  1343,  7009, 10296,\n",
       "          7149,    10, 18989,     4,     4,  3787,  7424,  7154,  7083, 15084,\n",
       "          8647,    10,  7009,  9319,   209, 17071, 11162, 10537,     4,  7446,\n",
       "         10407,  7338,   232,   218,    10,  3917,     4,    10,  5105,    10,\n",
       "          9785,     5,    10,  7279, 11542, 10296,   372,  7398,  7009,     4,\n",
       "         18378, 10460,    11,  7131, 15012,  7390,    11, 11723,  7489,     4,\n",
       "          9625,  1546,   225,    11, 13262,  9167,  8917,   120, 11802,     4,\n",
       "            11,  7046,  2558,  2133,  7406,  3832, 19281, 10995,  7748,    11,\n",
       "         10879,  3343,  3357,  9078,  2429,  7233,  2197,    10,  7401,  7341,\n",
       "          7339, 15733, 19889,  3076,  4280, 10552, 10109,  7536, 14899,     4,\n",
       "            11, 18989,  8854,  7670,  6652,  7005, 19540,  3094,  7154,  7078,\n",
       "          8781,  8842,  7312,   413,  4253,    10,  7262,    11,  7094, 16423,\n",
       "             4,     4,  7047, 10584,  1147,    11,  7660,  9999,     4,  1147,\n",
       "            10,  7047,  7204,  7102,  7430, 11394,    10,     4,  6633,  4052,\n",
       "          9584,    10,  2845,  9167, 14289,  7702, 14496, 10689,  6992, 11370,\n",
       "             4,  7003,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'decoder_input_ids': tensor([    5,    11, 10879,  7233,  2197,    10,  7401,  4280,  7009,  9117,\n",
       "          4280,  7341,  7339, 15733, 19889,  3076,  4280, 10552, 10109,  7536,\n",
       "         14899,    11,  6999,  7489,  8282,  7534,    10, 14437, 10638,  7861,\n",
       "            11, 10638,  7120,  4212,   176,   206,  7019,  7099,  7068,   177,\n",
       "          2505,  7013,  7192,    10,  7401, 12012,  4188,  4280,  4188,  4305,\n",
       "           232, 11852,  7215,    11,  7043,  7087, 10773,  9333, 18330,    10,\n",
       "          7707,  9541,    11,  7168,  3773,  2463,  4395,  1438,    10, 16189,\n",
       "          7091,  7168,  3773,  2463,  7130,  1121,  1121,   228,  1034,  7940,\n",
       "          4707,    11, 18989,  8854,  7670,  6652,  7005,  7154,  7078,  8781,\n",
       "          8842,   413,  4253,    10, 18989,  3787,  7424,  7154,  7083,  8647,\n",
       "          7279,  9814,    11, 16166,  4632,   248, 19957,    10,  7126,  7964,\n",
       "          4468,  5868,  4468,  7478,  1438,  4379,  5052, 14274,    11,  7175,\n",
       "          5045,  5094,   146,    10, 14437, 10296,  3917,  1809,    11,  7091,\n",
       "          8478,  5868, 16154,    10,   177,  1343,  7009, 10296,  7149,    11,\n",
       "         16166,  4632,  6999,  2224,  5025,  2126, 10337,  2126,  7251,  4476,\n",
       "          6128, 11612,  1592,    10, 11449, 17871,  9981,   146,    11,  7023,\n",
       "          4632,    10,  7103, 11067,   146,    11,  7075,  4489,    10,  7075,\n",
       "          4586,  4280,  7106, 12742,  1354,  7001, 17466,   177,  3323, 13083,\n",
       "            11,  7046,  2558,  2133,  7406,  3832, 10995,  7748,    10,  7279,\n",
       "         11542, 10296,   372,  7398,  7009, 18378, 10460,    11,  7099,  4253,\n",
       "           232, 12012,   232,   218,  9862,  4632,   248,    10,  7991,  4160,\n",
       "          8936,    11,  7047,  9104,  4607,    10,  7107,  7854,  7439, 10460,\n",
       "          7069,  7048, 19176,  4590,  7030,  9860,  1571,  7413,  9040, 11867,\n",
       "           611, 10374,    11,  7759,  6633, 19176,  1571,  7406,  3832, 11387,\n",
       "            10,  9441,   177,  3760, 12017,  6633,    11,  7464,  7685,  7084,\n",
       "          9102,  8682,  8258,  7062,  7544,    10,  6995,  6632,  6632, 11025,\n",
       "          7020,  8647,    11,  9167,  7143,  7384,  8917,   120,    10,  8063,\n",
       "          7501,  7683,  7186,  7088,  7493,  7016,    11,  7046, 15888,  7091,\n",
       "          7489,  9625,  1546,   225,    10,  7047, 16837,  7493,  7003,    11,\n",
       "          7492,  7206,  4648, 13752,    10, 11884,   128,   128,  7110,  5029,\n",
       "          7522,    11,  7131, 15012, 10275,  1540,  7742,  7390,    10,  7419,\n",
       "           442,  5105,  7009,  9140,  9837,  7726,  7796,    11,  7071,  2562,\n",
       "          5025,  7680,    10,  7880,  6633,  4052,  9584,    11,  7131,  8019,\n",
       "          7419,   567,  3888,  7298,  7295,   245,    10,  7102,  2846,  9878,\n",
       "          4696,  7108,  6992,  7514,   611,  2463,  8858, 11128,  7830,  7412,\n",
       "          6991,  7756,  8529, 18234,  9644,  7086,  7291,  8872,    11,  7047,\n",
       "          8936,  7389,    10,  9878,  4696,   225,  9117,  7068,  5474,  1418,\n",
       "          7107,  9057,   985,  2150, 18198,  8342,  7476,  2492,  7583,  1125,\n",
       "          9785,    11,  7046,  7010,  7143,  7010, 11656,  1053,  4271,  8224,\n",
       "            10,  7655,  7009,  9319,  4632,  7001,   209, 11162, 10537,  7446,\n",
       "         10407,  7338,  4188,  4324,   232,   218,    11,  7660,  9999,  1147,\n",
       "            10,  7484,  7003,  7031,  7132,  8686,  7098,  2093,    11,  7047,\n",
       "         12537,  1147,  7869,  9930, 17967,    10,  7047,  7204,  7102,  7430,\n",
       "         11394,  7756, 13946,    11,  7215,  7143,  7094,  7047, 10584,  1147,\n",
       "            10,  9785,    11,  7143,  7094,  7760,  8587,    10,  7056,  9245,\n",
       "          7049,  7471,   177,  1903,  1418,  7274,    11,  7023,  7143, 11956,\n",
       "          7692,    10,  7262,    11,  7121, 10130,   177,  4831,   283,  1438,\n",
       "          7204,  7647,  7196,  7143,   438,  7954,  7242, 15537,  8737,  7211,\n",
       "            10,  7240,  7121,  6313,   258,  8229,  7937, 14656,  1438,    11,\n",
       "          7090,  7257,  7023,  7056, 12250,  7155, 14496,  6633,  3573,  7389,\n",
       "            10,  9167,  7702, 14496, 10689,  6992, 11370,  7003,     0,     0,\n",
       "             0,     0]),\n",
       " 'decoder_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   11, 10879,  7233,  2197,    10,  7401,  4280,  7009,  9117,  4280,\n",
       "          7341,  7339, 15733, 19889,  3076,  4280, 10552, 10109,  7536, 14899,\n",
       "            11,  6999,  7489,  8282,  7534,    10, 14437, 10638,  7861,    11,\n",
       "         10638,  7120,  4212,   176,   206,  7019,  7099,  7068,   177,  2505,\n",
       "          7013,  7192,    10,  7401, 12012,  4188,  4280,  4188,  4305,   232,\n",
       "         11852,  7215,    11,  7043,  7087, 10773,  9333, 18330,    10,  7707,\n",
       "          9541,    11,  7168,  3773,  2463,  4395,  1438,    10, 16189,  7091,\n",
       "          7168,  3773,  2463,  7130,  1121,  1121,   228,  1034,  7940,  4707,\n",
       "            11, 18989,  8854,  7670,  6652,  7005,  7154,  7078,  8781,  8842,\n",
       "           413,  4253,    10, 18989,  3787,  7424,  7154,  7083,  8647,  7279,\n",
       "          9814,    11, 16166,  4632,   248, 19957,    10,  7126,  7964,  4468,\n",
       "          5868,  4468,  7478,  1438,  4379,  5052, 14274,    11,  7175,  5045,\n",
       "          5094,   146,    10, 14437, 10296,  3917,  1809,    11,  7091,  8478,\n",
       "          5868, 16154,    10,   177,  1343,  7009, 10296,  7149,    11, 16166,\n",
       "          4632,  6999,  2224,  5025,  2126, 10337,  2126,  7251,  4476,  6128,\n",
       "         11612,  1592,    10, 11449, 17871,  9981,   146,    11,  7023,  4632,\n",
       "            10,  7103, 11067,   146,    11,  7075,  4489,    10,  7075,  4586,\n",
       "          4280,  7106, 12742,  1354,  7001, 17466,   177,  3323, 13083,    11,\n",
       "          7046,  2558,  2133,  7406,  3832, 10995,  7748,    10,  7279, 11542,\n",
       "         10296,   372,  7398,  7009, 18378, 10460,    11,  7099,  4253,   232,\n",
       "         12012,   232,   218,  9862,  4632,   248,    10,  7991,  4160,  8936,\n",
       "            11,  7047,  9104,  4607,    10,  7107,  7854,  7439, 10460,  7069,\n",
       "          7048, 19176,  4590,  7030,  9860,  1571,  7413,  9040, 11867,   611,\n",
       "         10374,    11,  7759,  6633, 19176,  1571,  7406,  3832, 11387,    10,\n",
       "          9441,   177,  3760, 12017,  6633,    11,  7464,  7685,  7084,  9102,\n",
       "          8682,  8258,  7062,  7544,    10,  6995,  6632,  6632, 11025,  7020,\n",
       "          8647,    11,  9167,  7143,  7384,  8917,   120,    10,  8063,  7501,\n",
       "          7683,  7186,  7088,  7493,  7016,    11,  7046, 15888,  7091,  7489,\n",
       "          9625,  1546,   225,    10,  7047, 16837,  7493,  7003,    11,  7492,\n",
       "          7206,  4648, 13752,    10, 11884,   128,   128,  7110,  5029,  7522,\n",
       "            11,  7131, 15012, 10275,  1540,  7742,  7390,    10,  7419,   442,\n",
       "          5105,  7009,  9140,  9837,  7726,  7796,    11,  7071,  2562,  5025,\n",
       "          7680,    10,  7880,  6633,  4052,  9584,    11,  7131,  8019,  7419,\n",
       "           567,  3888,  7298,  7295,   245,    10,  7102,  2846,  9878,  4696,\n",
       "          7108,  6992,  7514,   611,  2463,  8858, 11128,  7830,  7412,  6991,\n",
       "          7756,  8529, 18234,  9644,  7086,  7291,  8872,    11,  7047,  8936,\n",
       "          7389,    10,  9878,  4696,   225,  9117,  7068,  5474,  1418,  7107,\n",
       "          9057,   985,  2150, 18198,  8342,  7476,  2492,  7583,  1125,  9785,\n",
       "            11,  7046,  7010,  7143,  7010, 11656,  1053,  4271,  8224,    10,\n",
       "          7655,  7009,  9319,  4632,  7001,   209, 11162, 10537,  7446, 10407,\n",
       "          7338,  4188,  4324,   232,   218,    11,  7660,  9999,  1147,    10,\n",
       "          7484,  7003,  7031,  7132,  8686,  7098,  2093,    11,  7047, 12537,\n",
       "          1147,  7869,  9930, 17967,    10,  7047,  7204,  7102,  7430, 11394,\n",
       "          7756, 13946,    11,  7215,  7143,  7094,  7047, 10584,  1147,    10,\n",
       "          9785,    11,  7143,  7094,  7760,  8587,    10,  7056,  9245,  7049,\n",
       "          7471,   177,  1903,  1418,  7274,    11,  7023,  7143, 11956,  7692,\n",
       "            10,  7262,    11,  7121, 10130,   177,  4831,   283,  1438,  7204,\n",
       "          7647,  7196,  7143,   438,  7954,  7242, 15537,  8737,  7211,    10,\n",
       "          7240,  7121,  6313,   258,  8229,  7937, 14656,  1438,    11,  7090,\n",
       "          7257,  7023,  7056, 12250,  7155, 14496,  6633,  3573,  7389,    10,\n",
       "          9167,  7702, 14496, 10689,  6992, 11370,  7003,     6,     0,     0,\n",
       "             0,     0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T07:15:06.850362Z",
     "start_time": "2022-03-16T07:15:06.843350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9241, 196, 181, 183, 187, 188, 196, 181, 185, 183]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    \"asdfjksdhfljkisdhfuisdhuifhsihfiwhfwuihfuwehfwuefiojofisdjoifjiosfs\",\n",
    "    max_length=10,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T07:09:54.839113Z",
     "start_time": "2022-03-16T07:09:54.823582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<user1> 넵 고용노동부??? 아아아ᅡ ᄋᄋᄋᄋᄋ 근데 그거 취업준비하는거라 하면 100퍼 취업해야하는거 아녀?? 아 그렇네 ᄆᄌ 그거 빡센데 할만하긴 하도라 ᄆᄌ 못하게해 ᄏᄏᄏ큐ᅲᅲᅲᅲ 그럼 유학은 어떻게 가? 아아 아아아 아핫~ 맞아 근데 유학 그때 뭐지 2학년마쳐야한다고 그랬어서 이젠 그거 필요없는거야? ᄏᄏᄏᄏᄏᄏ 호올 글쿤 걔네 위치 근처에있엉?? 오 대박 다행쓰~ 괜찮네여 굳굳 그럼 내일도 못볼각인가요,,.,.? 마자 돈도 주니까 ᄏᄏᄏᄏᄏᄏ 네에..? 아ᅥᄆ마... 엄마...... 그랬으면 좋겟군여.. ᄏᄏᄏᄏᄏᄏᄏᄏᄏ 방학인데 왜... 왜죠..? ᄏᄏᄏᄏᄏᄏᄏᄏᄏ 나도 괜찮아 우선 거기서 어쩌피 알바못하게하는거니까 아 내가 듣기엔 막 내가 출석을 빠지거나 이러면 자기가 메꾼다는식으로 들었던거같기도해서 지원해주는거라 빠지면 안된다하두라 오호... 싫다 ᄏᄏᄏᄏᄏ ᄋᄒᄋᄒᄋᄒ 아아 그런식이구나 ᄏᄏᄏᄏᄏ 한달코스면 너 거기서 살아야할걸 방학인데 또 못본다 ᄏᄏᄏᄏᄏᄏ ᄏᄏᄏᄏᄏᄏ켄엣? 네엣? ᄏᄏᄏᄏᄏᄏ 너무 슬프구요.. 아 이번에 다 끝내려고? ᄋ아아 멋지다 ᄆᄌᄆᄌ 맞긴하지 이제 살도 많이 뺏구 건강유지하는게 좋긴행 마자 어쩌피 피티하면 빠지겟다 남은거 하다보면 ᄏᄏᄏᄏᄏᄏ 아그곃 ᄆ극혐 쉬발탱 그래도 부족하다니 ᄃᄃᄃ 무섭구나 그니까 그래 배우는걸 확실히 배우면서 다니는게 낫징 굳굳 전 좋아요 멋진계획입니다 큘 ᅲᅲᅲᅲᅲᅲᅲᅲᅲ 싫다요~~~~~~~~ 야다요~~~ 아니면 아 어쩌피 취업해야하니까 아님 그 그거해서 취업하고나서 좀만 다니고 내년 2학기엔가 진주인가 현준가 누구랑 할수도 있다고 하지않았니 2학기 그런식으로 하면 어땡 ᄏᄏᄏᄏᄏ진주도 현주도 아니고 name16이.. ᄏᄏᄏᄏᄏᄏᄏ ᄏᄏᄏᄏᄏᄏᄏᄏname8넼ᄏ ᄏᄏᄏᄏᄏᄏᄏᄏ asmr zᄏᄏᄏᄏᄏ 오? 동기인데 왕십리까지옴? 헉어때 아미친 ᄏᄏᄏᄏᄏᄏᄏ 실밥풀기전 뭔데 ᄏᄏᄏᄏᄏᄏ 아니 너무 아니 시기를 어느정도 가려야지 실밥도 안풀고 나오냐곸ᄏᄏᄉᄇ 헐 어떻데 나돈데 나랑 똑같네 ᄏᄏᄏᄏᄏᄏᄏᄏᄏ ᄏᄏᄏ나도 할래 아아 나도 눈매교정 필요해 청담이라서?? 미친 <unk><unk><unk><unk><unk> 미쳣다리 ᄆᄌ 보통 100얼망 언저리 함 부럽다.. 나도 엄마가 맨날 내가 하고싶다고 해서 해준다곤 햇는데 막상 하려하면 내가 무섭고.. ᄏᄏᄏᄏᄏᄏᄏ 그리고 아빠는 자꾸 무쌍의 시대가 온다고 존버하라는거야 ᄉᄇ ᄏᄏᄏᄏᄏ 아니 존버할 눈은 따로있구유ᅧᅲᅲᅲ 약간 뷔같이 무쌍인데 예쁜눈 있자늠 ᄏᄏᄏᄏᄏ 난 아냐 ᄏᄏᄏᄏᄏ ᄂᄃ예전에 ᄏᄏᄏᄆᄌᄆᄌ 아 지금 일하면서 하는거라 ᄀᄎᄀᄎ ᄏᄏᄏᄏᄏᄏ ᄏᄏᄏᄏ캐ᅣ캬럌럌ᄅ ᄏᄏᄏᄏᄏ 아 배고파 너 저녁 빨리 먹어야하지않아? 그러네 게다가7시까지 마자 너 집오면 허기질거까튼데 ᄏᄏᄏᄏᄏ 오엠지ᅲᅲᅲ ᅲᅲᅲᅲᅲᅲᅲ ᄏᄏᄏᄏᄏᄏᄏ 나도 배고파ᅲ 나 분명 알바올때는 아별로 배 안고프네 다이어트 할수있겠다 이생각이었는데 지금되니까 ᄌᄅ배고픔 ᄏᄏ<unk><unk>..?????? 그거 name10오빠랑 name19언니랑 하는거??????? ᄋᄋᄋ ᄏᄏᄏᄏ백두쓰 ᄋᄋᄋ 차도 있어서? ᄋᄋᄏᄏᄏᄏ 금요일날 하는거징 나 근데 토요일날 약속있을지두 몰라 오미친 벌써라니 취소하겟ᄉ븐다 ᄏᄏᄏᄏᄏᄏᄏᄏᄏ 아냐 너네 언니 가면되지 웅 ᄀᄎ 우선 그거 약속도 확정아니라서 되는애들 있는지 물어보자 ᄆᄌᄆᄌ 너는 어쩌피 앜ᄏᄏᄏᄏ 옼에.. 가면 너는 어디까지 하고온댓지? 아아 ᄋᄏᄋᄏ ᄏᄏᄏᄏᄏ 용돈쓰~!~!!! ᄆᄌ 가야한다 그건 가야한다요 ᄏᄏᄏᄏᄏᄏᄏ 아 언니 몇시였죠 아 나 그거 대외활동하잖아 까먹었네 쓰바 ᄏᄏᄏᄏᄏ 근데 1시반 에 끝날수도있으니까 만약 시간안맞으면 대충 빼지모 ᄒᄉᄒ ᄏᄏᄏᄏᄏᄏᄏ 엉엉 장소변경되서 혹시 모르는데 그래도 그때로... 기도하여봅니다.. 아헠ᄏᄏᄏᄏ 귀여우셩 ᅲᅲ 누가 에어컨 틀으세요 ᄏᄏᄏᄏᄏᄏᄏᄏ 아헐 가지뫄요.. 너무 이르다 나 너무 배고프자나요 나랑 밥먹고가 ᅲᅮ ᄏᄏᄏᄏᄏ귀여우셔 ᄏᄏᄏᄏᄏ 네엣? ᄏᄏᄏᄏᄏᄏᄏᄏ 네? 모르겠어요 ᄏᄏᄏᄏᄏᄏᄏᄏ ᄏᄏᄏᄏ근데 웃긴거 나도 4분뒤면 퇴근이야 앜ᄏᄏᄏ 오늘 어쩌피 나 대외활동 그거 과제있어서 밥을 챙겨먹기도 애매해서 ᅲᅲᅲᅲ만나는건아니고 자료조사 ᄏᄏᄏᄏᄏᄏᄏᄏ 근데 좀 디테일하게 해달라해서 움 구건아닌데 걍 밥을 밖에서 먹을까 생각하고있엉 그칭 역시 혼다라멘인가 ᄏᄏᄏᄏᄏ ᄏᄏᄏᄏᄏ몇시에 일어난다고 그래서? 나는 사실 내일은 뭐 특별한거 없고 알바하고 게임할라햇엉 요 며칠 게임달린다했자나 ᄏᄏᄏᄏᄏᄏ 그래서 알바갔다가 너 카페알바갈때 나는 피방갈라햇다 ᄏᄏᄏᄏᄏᄏᄏᄏᄏ앜ᄏᄏᄏ 오늘 나 수영도 가야해 하필 오늘 좀 몰려있다 네넹~! 나 잠깐 퇴근준비할겡<eos>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5944\n",
    "print(len(train_dataset[i]['labels']))\n",
    "tokenizer.decode(train_dataset[i]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T07:08:24.035075Z",
     "start_time": "2022-03-16T07:08:00.859661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650136e5f14047c3a036d94fdd9025f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1445279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4555 decoder_input_ids\n",
      "4555 decoder_attention_mask\n",
      "4555 labels\n",
      "4997 decoder_input_ids\n",
      "4997 decoder_attention_mask\n",
      "4997 labels\n",
      "5944 decoder_input_ids\n",
      "5944 decoder_attention_mask\n",
      "5944 labels\n",
      "5960 decoder_input_ids\n",
      "5960 decoder_attention_mask\n",
      "5960 labels\n",
      "6077 decoder_input_ids\n",
      "6077 decoder_attention_mask\n",
      "6077 labels\n",
      "7088 decoder_input_ids\n",
      "7088 decoder_attention_mask\n",
      "7088 labels\n",
      "7437 decoder_input_ids\n",
      "7437 decoder_attention_mask\n",
      "7437 labels\n",
      "7439 decoder_input_ids\n",
      "7439 decoder_attention_mask\n",
      "7439 labels\n",
      "7444 decoder_input_ids\n",
      "7444 decoder_attention_mask\n",
      "7444 labels\n",
      "7446 decoder_input_ids\n",
      "7446 decoder_attention_mask\n",
      "7446 labels\n",
      "7447 decoder_input_ids\n",
      "7447 decoder_attention_mask\n",
      "7447 labels\n",
      "7449 decoder_input_ids\n",
      "7449 decoder_attention_mask\n",
      "7449 labels\n",
      "7451 decoder_input_ids\n",
      "7451 decoder_attention_mask\n",
      "7451 labels\n",
      "7454 decoder_input_ids\n",
      "7454 decoder_attention_mask\n",
      "7454 labels\n",
      "7456 decoder_input_ids\n",
      "7456 decoder_attention_mask\n",
      "7456 labels\n",
      "7459 decoder_input_ids\n",
      "7459 decoder_attention_mask\n",
      "7459 labels\n",
      "7462 decoder_input_ids\n",
      "7462 decoder_attention_mask\n",
      "7462 labels\n",
      "7464 decoder_input_ids\n",
      "7464 decoder_attention_mask\n",
      "7464 labels\n",
      "7468 decoder_input_ids\n",
      "7468 decoder_attention_mask\n",
      "7468 labels\n",
      "7470 decoder_input_ids\n",
      "7470 decoder_attention_mask\n",
      "7470 labels\n",
      "7472 decoder_input_ids\n",
      "7472 decoder_attention_mask\n",
      "7472 labels\n",
      "7474 decoder_input_ids\n",
      "7474 decoder_attention_mask\n",
      "7474 labels\n",
      "7476 decoder_input_ids\n",
      "7476 decoder_attention_mask\n",
      "7476 labels\n",
      "7478 decoder_input_ids\n",
      "7478 decoder_attention_mask\n",
      "7478 labels\n",
      "7480 decoder_input_ids\n",
      "7480 decoder_attention_mask\n",
      "7480 labels\n",
      "7484 decoder_input_ids\n",
      "7484 decoder_attention_mask\n",
      "7484 labels\n",
      "7487 decoder_input_ids\n",
      "7487 decoder_attention_mask\n",
      "7487 labels\n",
      "7490 decoder_input_ids\n",
      "7490 decoder_attention_mask\n",
      "7490 labels\n",
      "7492 decoder_input_ids\n",
      "7492 decoder_attention_mask\n",
      "7492 labels\n",
      "7497 decoder_input_ids\n",
      "7497 decoder_attention_mask\n",
      "7497 labels\n",
      "7504 decoder_input_ids\n",
      "7504 decoder_attention_mask\n",
      "7504 labels\n",
      "7506 decoder_input_ids\n",
      "7506 decoder_attention_mask\n",
      "7506 labels\n",
      "7508 decoder_input_ids\n",
      "7508 decoder_attention_mask\n",
      "7508 labels\n",
      "7509 decoder_input_ids\n",
      "7509 decoder_attention_mask\n",
      "7509 labels\n",
      "7511 decoder_input_ids\n",
      "7511 decoder_attention_mask\n",
      "7511 labels\n",
      "7518 decoder_input_ids\n",
      "7518 decoder_attention_mask\n",
      "7518 labels\n",
      "7521 decoder_input_ids\n",
      "7521 decoder_attention_mask\n",
      "7521 labels\n",
      "7523 decoder_input_ids\n",
      "7523 decoder_attention_mask\n",
      "7523 labels\n",
      "7525 decoder_input_ids\n",
      "7525 decoder_attention_mask\n",
      "7525 labels\n",
      "7527 decoder_input_ids\n",
      "7527 decoder_attention_mask\n",
      "7527 labels\n",
      "7529 decoder_input_ids\n",
      "7529 decoder_attention_mask\n",
      "7529 labels\n",
      "7532 decoder_input_ids\n",
      "7532 decoder_attention_mask\n",
      "7532 labels\n",
      "7988 decoder_input_ids\n",
      "7988 decoder_attention_mask\n",
      "7988 labels\n",
      "8392 decoder_input_ids\n",
      "8392 decoder_attention_mask\n",
      "8392 labels\n",
      "8414 decoder_input_ids\n",
      "8414 decoder_attention_mask\n",
      "8414 labels\n",
      "9815 decoder_input_ids\n",
      "9815 decoder_attention_mask\n",
      "9815 labels\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/envs/main/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32074/3507451022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nas_storage3/nsw0311/workspace/DialogBart/dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nas_storage3/nsw0311/workspace/DialogBart/dataloader.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         e = self.tokenizer.pad(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/envs/main/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2840\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     def create_token_type_ids_from_sequences(\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/envs/main/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/envs/main/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    721\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     )\n\u001b[0;32m--> 723\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    724\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                     \u001b[0;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "for i in trange(len(train_dataset)):\n",
    "    data = train_dataset[i]\n",
    "    for k, v in data.items():\n",
    "        if len(v) != 512:\n",
    "            print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T05:54:08.409451Z",
     "start_time": "2022-03-15T05:54:07.837476Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"d_model\": 256,\n",
    "    \"encoder_layers\": 6,\n",
    "    \"decoder_layers\": 6,\n",
    "    \"encoder_attention_heads\": 8,\n",
    "    \"decoder_attention_heads\": 8,\n",
    "    \"encoder_ffn_dim\": 1024,\n",
    "    \"decoder_ffn_dim\": 1024,\n",
    "    \"activation_function\": \"gelu\",\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"decoder_start_token_id\": tokenizer.bos_token_id,\n",
    "    \"forced_eos_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "model = BartForConditionalGeneration(BartConfig(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T05:54:09.069994Z",
     "start_time": "2022-03-15T05:54:08.411329Z"
    }
   },
   "outputs": [],
   "source": [
    "r = model(**train_dataset[:4])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf4433abc8e752da9bbc01c74f7a97c74d3a41f419f8f50f7e9bcce58e5fc3eb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
